#데이터 전처리 할 때 familysize 값 최대 12로 맞춘 모델
#LGBM 결과
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import warnings
import xgboost as xgb
warnings.filterwarnings('ignore')
%matplotlib inline

from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from xgboost import plot_importance
from sklearn.datasets import load_breast_cancer
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

#데이터 불러오기
train=pd.read_csv("train.csv")
test=pd.read_csv("test.csv")

#결손값 처리
zero_features = ['education', 'engnat', 'hand', 'married']

index = test['index']

#결손값(무응답 = 0)제거 함수
def drop_NoResponse(df):
    for col in zero_features:
        df.drop(df[df[col]==0].index, axis=0, inplace=True)
        
    return df

#train, test에서 결손값 제거
train = drop_NoResponse(train)

pd.set_option('display.max_columns', 100)

Unneed_list = ['QaE', 'QbE', 'QcE', 'QdE', 'QeE','QfE', 'QgE', 'QhE', 'QiE', 'QjE',
               'QkE', 'QlE', 'QmE', 'QnE', 'QoE','QpE', 'QqE', 'QrE', 'QsE', 'QtE', 'index']

train.drop(Unneed_list, axis=1, inplace=True)
test.drop(Unneed_list, axis=1, inplace=True)


train=pd.get_dummies(train)
test=pd.get_dummies(test)

##familysize 값이 12보다 큰 값을 12로 맞춤

train.loc[train['familysize'] > 12, 'familysize'] = 12

#trainX에 투표값 제거
train_X = train.drop('voted', axis = 1)

#trainy에 투표값만 넣음
train_y = train['voted']

X_train, X_test, y_train, y_test=train_test_split(train_X, train_y, test_size=0.2, random_state=0)

lgbm_wrapper = LGBMClassifier(n_estimators=400)

evals = [(X_test, y_test)]
lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric='logloss',
                eval_set=evals, verbose=True)
preds = lgbm_wrapper.predict(X_test)


def get_clf_eval(y_test, pred):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    print('오차행렬')
    print(confusion)
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}'.format(accuracy, precision, recall, f1))
    
get_clf_eval(y_test, preds)
X_new = test
X_new.shape
new_pred_class = lgbm_wrapper.predict(X_new)
new_pred_class.shape
pred_proba = lgbm_wrapper.predict_proba(X_new)[:,1]
sub = pd.DataFrame({'index':index, 'pred_prob':pred_proba, 'pred': new_pred_class})
sub.set_index('index', inplace=True)
sub.to_csv('lgbm_sum1.csv')
